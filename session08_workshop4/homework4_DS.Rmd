---
title: "Homework 4: Machine Learning"
author: "Divij Nandan Sharma"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
options(scipen = 999) #disable scientific notation
library(tidyverse)
library(tidymodels)
library(GGally)
library(sf)
library(leaflet)
library(janitor)
library(rpart.plot)
library(here)
library(scales)
library(vip)
library(C50)
```

# The Bechdel Test

<https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/>

The [Bechdel test](https://bechdeltest.com) is a way to assess how women are depicted in Hollywood movies. In order for a movie to pass the test:

1.  It has to have at least two [named] women in it
2.  Who talk to each other
3.  About something besides a man

There is a nice article and analysis you can find here <https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/> We have a sample of 1394 movies and we want to fit a model to predict whether a film passes the test or not.

```{r read_data}

bechdel <- read_csv(here::here("data", "bechdel.csv")) %>% 
  mutate(test = factor(test)) 
glimpse(bechdel)

```

How many films fail/pass the test, both as a number and as a %?

```{r}
# Calculate the number of films that pass and fail the test
test_summary <- bechdel %>%
  group_by(test) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

# Display the result
print(test_summary)

```

## Movie scores

```{r}
ggplot(data = bechdel, aes(
  x = metascore,
  y = imdb_rating,
  colour = test
)) +
  geom_point(alpha = .3, size = 3) +
  scale_colour_manual(values = c("tomato", "olivedrab")) +
  labs(
    x = "Metacritic score",
    y = "IMDB rating",
    colour = "Bechdel test"
  ) +
 theme_light()
```

# Split the data

```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(bechdel, # updated data
                           prop = 0.8, 
                           strata = test)

bechdel_train <- training(data_split) 
bechdel_test <- testing(data_split)
```

Check the counts and % (proportions) of the `test` variable in each set.

```{r}
# Function to calculate counts and proportions
calculate_proportions <- function(data) {
  data %>%
    group_by(test) %>%
    summarise(count = n()) %>%
    mutate(percentage = count / sum(count) * 100)
}

# Calculate counts and proportions for the training set
train_proportions <- calculate_proportions(bechdel_train)
print("Training Set Proportions")
print(train_proportions)

# Calculate counts and proportions for the testing set
test_proportions <- calculate_proportions(bechdel_test)
print("Testing Set Proportions")
print(test_proportions)
```

## Feature exploration

## Any outliers?

```{r}

bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore) %>% 

    pivot_longer(cols = 2:6,
               names_to = "feature",
               values_to = "value") %>% 
  ggplot()+
  aes(x=test, y = value, fill = test)+
  coord_flip()+
  geom_boxplot()+
  facet_wrap(~feature, scales = "free")+
  theme_bw()+
  theme(legend.position = "none")+
  labs(x=NULL,y = NULL)

```

## Scatterplot - Correlation Matrix

Write a paragraph discussing the output of the following

```{r, warning=FALSE, message=FALSE}
bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore)%>% 
  ggpairs(aes(colour=test), alpha=0.2)+
  theme_bw()
```
Key Observations:
Budget vs. Gross Revenues (Domestic and International):

There is a strong positive correlation between budget_2013 and both domgross_2013 (0.624) and intgross_2013 (0.938). This indicates that films with higher budgets tend to have higher domestic and international gross revenues.
When separated by Bechdel test results, both passing and failing films show similar positive correlations, but passing films exhibit slightly stronger correlations.
Gross Revenues (Domestic vs. International):

There is an extremely high correlation (0.956) between domgross_2013 and intgross_2013, suggesting that films that perform well domestically also tend to perform well internationally.
This relationship holds true for both passing and failing films, though the correlation is slightly higher for passing films.
Ratings and Scores:

The imdb_rating and metascore are moderately positively correlated (0.737). Films that receive higher IMDb ratings also tend to receive higher Metascores.
This positive correlation is consistent across both passing and failing films, with passing films showing a slightly higher correlation.
Correlation with Bechdel Test:

Films that pass the Bechdel test tend to have slightly higher correlations between budget and gross revenues, as well as between gross revenues and ratings/scores.
The distribution of imdb_rating and metascore shows that films passing the Bechdel test tend to have slightly higher average ratings and scores compared to those that fail.

Implications:
The strong correlations between budget and gross revenues indicate that financial investment in films is a significant predictor of their box office performance.
The consistent positive relationship between domestic and international gross revenues suggests that successful films in one market are likely to succeed in others.
Higher IMDb ratings and Metascores for passing films may imply that audiences and critics respond more positively to films that pass the Bechdel test, reflecting a possible preference for films with better female representation.

## Categorical variables

Write a paragraph discussing the output of the following

```{r}
bechdel %>% 
  group_by(genre, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
  
 
bechdel %>% 
  group_by(rated, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
```
Analysis by Genre
The first analysis groups the movies by genre and test.

Observations:

Comedies and dramas have a relatively higher proportion of movies passing the Bechdel test.
Genres like action, adventure, and animation tend to have higher proportions of movies failing the test.
All movies categorized as documentaries, musicals, sci-fi, and thrillers pass the Bechdel test, though the sample sizes for these genres are small.

The second analysis groups the movies by rated (movie rating) and test.

Observations:

Movies rated NC-17 have the highest proportion of failures in the Bechdel test.
G-rated movies also tend to fail the test more often than they pass.
PG-13 and R-rated movies have a roughly even distribution, with a slight tendency towards failing the test.
PG-rated movies fail the test more frequently than they pass.

Overall Conclusion
The analysis indicates that certain genres and ratings are more likely to fail the Bechdel test. Specifically, action, adventure, animation, and crime genres, as well as NC-17 rated movies, have higher proportions of failing the Bechdel test. Conversely, genres such as comedy, drama, and horror have a better record of passing the test. These insights can inform future film productions, highlighting the need for more inclusive representation in specific genres and rating categories.

# Train first models. `test ~ metascore + imdb_rating`

```{r}
lr_mod <- logistic_reg() %>% 
  set_engine(engine = "glm") %>% 
  set_mode("classification")

lr_mod


tree_mod <- decision_tree() %>% 
  set_engine(engine = "C5.0") %>% 
  set_mode("classification")

tree_mod 
```

```{r}


lr_fit <- lr_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )

tree_fit <- tree_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )
```

## Logistic regression

```{r}
lr_fit %>%
  broom::tidy()

lr_preds <- lr_fit %>%
  augment(new_data = bechdel_train) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0))

```

### Confusion matrix

```{r}
lr_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")


```

## Decision Tree

```{r}
tree_preds <- tree_fit %>%
  augment(new_data = bechdel) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0)) 


```

```{r}
tree_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

## Draw the decision tree

```{r, fig.width=10, fig.height=7}
draw_tree <- 
    rpart::rpart(
        test ~ metascore + imdb_rating,
        data = bechdel_train, # uses data that contains both birth weight and `low`
        control = rpart::rpart.control(maxdepth = 5, cp = 0, minsplit = 10)
    )
#    partykit::as.party()
#plot(draw_tree)

# Plot the tree using rpart.plot
rpart.plot(draw_tree, 
           type = 3, # type of plot: 3 for a more detailed plot
           extra = 104, # display the number of observations and the percentage
           under = TRUE, # display the extra text under the box
           fallen.leaves = TRUE, # place leaf nodes at the bottom of the plot
           main = "Decision Tree for Bechdel Test",
           box.palette = "RdBu", # color palette for the boxes
           shadow.col = "gray", # shadow color
           nn = TRUE) # display the node numbers
```

# Cross Validation

Run the code below. What does it return?

```{r}
set.seed(123)
bechdel_folds <- vfold_cv(data = bechdel_train, 
                          v = 10, 
                          strata = test)
bechdel_folds
```
The code creates a 10-fold cross-validation object using the vfold_cv function from the rsample package, stratified by the test variable. 

The bechdel_folds object returned by this code is a tibble with 10 rows and 2 columns:

splits: A list of the training and testing indices for each fold.
id: The identifier for each fold (e.g., "Fold01", "Fold02", etc.).
This object is used in model training to perform cross-validation, helping to evaluate the model's performance more robustly by ensuring that each data point is used for both training and validation.

## `fit_resamples()`

Trains and tests a resampled model.

```{r}
lr_fit <- lr_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )


tree_fit <- tree_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )
```

## `collect_metrics()`

Unnest the metrics column from a tidymodels `fit_resamples()`

```{r}

collect_metrics(lr_fit)
collect_metrics(tree_fit)


```

```{r}
tree_preds <- tree_mod %>% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds,
    control = control_resamples(save_pred = TRUE) #<<
  )

# What does the data for ROC look like?
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail)  

# Draw the ROC
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail) %>% 
  autoplot()

```

# Build a better training set with `recipes`

## Preprocessing options

-   Encode categorical predictors
-   Center and scale variables
-   Handle class imbalance
-   Impute missing data
-   Perform dimensionality reduction
-   ... ...

## To build a recipe

1.  Start the `recipe()`
2.  Define the variables involved
3.  Describe **prep**rocessing [step-by-step]

## Collapse Some Categorical Levels

Do we have any `genre` with few observations? Assign genres that have less than 3% to a new category 'Other'

```{r}
#| echo = FALSE
bechdel %>% 
  count(genre) %>% 
  mutate(genre = fct_reorder(genre, n)) %>% 
  ggplot(aes(x = genre, 
             y = n)) +
  geom_col(alpha = .8) +
  coord_flip() +
  labs(x = NULL) +
  geom_hline(yintercept = (nrow(bechdel_train)*.03), lty = 3)+
  theme_light()
```

```{r}
movie_rec <-
  recipe(test ~ .,
         data = bechdel_train) %>%
  
  # Genres with less than 5% will be in a catewgory 'Other'
    step_other(genre, threshold = .03) 
```

## Before recipe

```{r}
#| echo = FALSE
bechdel_train %>% 
  count(genre, sort = TRUE)
```

## After recipe

```{r}
movie_rec %>% 
  prep() %>% 
  bake(new_data = bechdel_train) %>% 
  count(genre, sort = TRUE)
```

## `step_dummy()`

Converts nominal data into numeric dummy variables

```{r}
#| results = "hide"
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_dummy(all_nominal_predictors()) 

movie_rec 
```

## Let's think about the modelling

What if there were no films with `rated` NC-17 in the training data?

-   Will the model have a coefficient for `rated` NC-17?
-   What will happen if the test data includes a film with `rated` NC-17?

## `step_novel()`

Adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set.

```{r}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal_predictors) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal_predictors()) 

```

## `step_zv()`

Intelligently handles zero variance variables (variables that contain only a single value)

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes()) 
  
```

## `step_normalize()`

Centers then scales numeric variable (mean = 0, sd = 1)

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) 

```

## `step_corr()`

Removes highly correlated variables

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) %>% 
  step_corr(all_predictors(), threshold = 0.75, method = "spearman") 



movie_rec
```

# Define different models to fit

```{r}
## Model Building

# 1. Pick a `model type`
# 2. set the `engine`
# 3. Set the `mode`: regression or classification

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

# Bundle recipe and model with `workflows`

```{r}
log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(movie_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow


## A few more workflows

tree_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(knn_spec)

```

HEADS UP

1.  How many models have you specified?
A.  The code specifies five models:

  Logistic Regression (log_spec)
  Decision Tree (tree_spec)
  Random Forest (rf_spec)
  Boosted Tree (xgb_spec)
  K-nearest Neighbor (knn_spec)

2.  What's the difference between a model specification and a workflow?
A.  Model Specification:

    A model specification defines the type of model you are going to use (e.g., logistic regression, decision tree) and sets its engine and mode. It includes parameters specific to the model and the engine     it uses.
    Example: log_spec, tree_spec, rf_spec, xgb_spec, knn_spec.
    Workflow:

    A workflow combines a model specification with a preprocessing recipe and specifies how the data should be prepared and how the model should be trained. It is a convenient way to bundle together all       the steps required to fit a model, including data preprocessing and model fitting.    
    Example: log_wflow, tree_wflow, rf_wflow, xgb_wflow, knn_wflow.  
    
3.  Do you need to add a formula (e.g., `test ~ .`) if you have a recipe?
A.  No, you do not need to add a formula if you have a recipe. The recipe already specifies how to preprocess the data and which variables to use. When you add a recipe to a workflow, it handles the data      preprocessing, and the model fitting will use the preprocessed data as defined by the recipe. The formula is not needed because the recipe defines the preprocessing steps, including any variable           selections and transformations.

# Model Comparison

You now have all your models. Adapt the code from slides `code-from-slides-CA-housing.R`, line 400 onwards to assess which model gives you the best classification.

```{r}


# Fit the models using cross-validation
log_res <- fit_resamples(log_wflow, resamples = bechdel_folds, metrics = metric_set(accuracy))
tree_res <- fit_resamples(tree_wflow, resamples = bechdel_folds, metrics = metric_set(accuracy))
rf_res <- fit_resamples(rf_wflow, resamples = bechdel_folds, metrics = metric_set(accuracy))
xgb_res <- fit_resamples(xgb_wflow, resamples = bechdel_folds, metrics = metric_set(accuracy))
knn_res <- fit_resamples(knn_wflow, resamples = bechdel_folds, metrics = metric_set(accuracy))

# Collect results
log_metrics <- collect_metrics(log_res)
tree_metrics <- collect_metrics(tree_res)
rf_metrics <- collect_metrics(rf_res)
xgb_metrics <- collect_metrics(xgb_res)
knn_metrics <- collect_metrics(knn_res)

# Compare the results
results <- bind_rows(
  log_metrics %>% mutate(model = "Logistic Regression"),
  tree_metrics %>% mutate(model = "Decision Tree"),
  rf_metrics %>% mutate(model = "Random Forest"),
  xgb_metrics %>% mutate(model = "Boosted Tree"),
  knn_metrics %>% mutate(model = "KNN")
)

print(results)
```

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (Rmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: N/A
-   Approximately how much time did you spend on this problem set: ~2hrs
-   What, if anything, gave you the most trouble: N/A

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?
Yes

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
