---
title: "Final Group project"
author: "Group 3"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(skimr)
library(kknn)
library(here)
library(tictoc)
library(vip)
library(ranger)
library(GGally)
```

# The problem: predicting credit card fraud

The goal of the project is to predict fraudulent credit card transactions.

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no?

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox <https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0> and save it in your `dsb` repo, under the `data` folder.

As we will be building a classifier model using tidymodels, there's two things we need to do:

1.  Define the outcome variable `is_fraud` as a factor, or categorical, variable, instead of the numerical 0-1 varaibles.
2.  In tidymodels, the first level is the event of interest. If we leave our data as is, `0` is the first level, but we want to find out when we actually did (`1`) have a fraudulent transaction

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv")) %>% 

  mutate(
    # in tidymodels, outcome should be a factor  
    is_fraud = factor(is_fraud),
    
    # first level is the event in tidymodels, so we need to reorder
    is_fraud = relevel(is_fraud, ref = "1")
         )

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

We also add some of the variables we considered in our EDA for this dataset during homework 2.

```{r}
card_fraud <- card_fraud %>% 
  mutate( hour = hour(trans_date_trans_time),
          wday = wday(trans_date_trans_time, label = TRUE),
          month_name = month(trans_date_trans_time, label = TRUE),
          age = interval(dob, trans_date_trans_time) / years(1)
) %>% 
  rename(year = trans_year) %>% 
  
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

```


## Exploratory Data Analysis (EDA)

You have done some EDA and you can pool together your group's expertise in which variables to use as features. You can reuse your EDA from earlier, but we expect at least a few visualisations and/or tables to explore teh dataset and identify any useful features.

```{r}
# There are a few very large transactions that affect how the histogram displays, I will first remove the outliers then create the histogram so that it displays properly
# Calculate the interquartile range (IQR) for transaction amounts
iqr_values <- card_fraud %>%
  summarise(
    Q1 = quantile(amt, 0.25),
    Q3 = quantile(amt, 0.75)
  ) %>%
  mutate(IQR = Q3 - Q1, Lower_Bound = Q1 - 1.5 * IQR, Upper_Bound = Q3 + 1.5 * IQR)

# Filter out outlier transaction amounts beyond the lower and upper bounds
filtered_data <- card_fraud %>%
  filter(amt >= iqr_values$Lower_Bound & amt <= iqr_values$Upper_Bound)

# Histogram of transaction amounts for the filtered data
ggplot(filtered_data, aes(x = amt, fill = factor(is_fraud))) +
  geom_histogram(bins = 50, alpha = 0.6, position = "identity") +
  scale_fill_manual(values = c("red", "blue"), labels = c("Fraudulent", "Legitimate")) +
  
# Display legit and fraud transactions separately to account for different frequencies and adjust the scale accordingly
  facet_wrap(~is_fraud, scales = "free") +
  labs(fill = "Transaction Type",
       x = "Transaction Amount ($)",
       y = "Frequency",
       title = "Distribution of Transaction Amounts (Outliers Removed)",
       subtitle = "Blue: Legitimate, Red: Fraudulent") +
  theme_bw() +
  NULL
```

```{r}
# Histograms for continuous variables
card_fraud %>%
  select(amt, city_pop, lat, long, age, distance_miles, distance_km) %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Continuous Variables")

```
```{r}
# Boxplots for continuous variables grouped by is_fraud
card_fraud %>%
  select(is_fraud, amt, city_pop, lat, long, age, distance_miles, distance_km) %>%
  gather(key = "variable", value = "value", -is_fraud) %>%
  ggplot(aes(x = is_fraud, y = value, fill = is_fraud)) +
  geom_boxplot() +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal() +
  labs(title = "Boxplots of Continuous Variables by Fraud Status")
```

```{r}
# Correlation matrix
continuous_vars <- card_fraud %>%
  select(amt, city_pop, lat, long, age, distance_miles, distance_km)

correlation_matrix <- cor(continuous_vars)

# Visualize correlation matrix
ggcorrplot::ggcorrplot(correlation_matrix, lab = TRUE, hc.order = TRUE, type = "lower")

```

```{r}
# Proportion of fraud by category
card_fraud %>%
  group_by(category, is_fraud) %>%
  summarise(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = category, y = prop, fill = is_fraud)) +
  geom_col(position = "stack") +  
  theme_minimal() +
  labs(title = "Proportion of Fraud by Category", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels

```

```{r}
# Number of transactions over time
card_fraud %>%
  mutate(trans_date = as.Date(trans_date_trans_time)) %>%
  group_by(trans_date) %>%
  summarise(transactions = n()) %>%
  ggplot(aes(x = trans_date, y = transactions)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Number of Transactions Over Time", x = "Date", y = "Number of Transactions")
```

```{r}

# Enhance data with additional time components
card_fraud_clean <- card_fraud %>%
  mutate(
    date_only = lubridate::date(trans_date_trans_time),
    month_name = lubridate::month(trans_date_trans_time, label = TRUE),
    hour = lubridate::hour(trans_date_trans_time),
    weekday = lubridate::wday(trans_date_trans_time, label = TRUE, week_start = 1)
  )

# Analyze fraud by month
fraud_by_month <- card_fraud_clean %>%
  mutate(is_fraud = as.numeric(as.character(is_fraud))) %>% 
  group_by(month_name) %>%
  summarise(
    Total_Transactions = n(),
    Fraud_Transactions = sum(is_fraud),
    Fraud_Percentage = (Fraud_Transactions / Total_Transactions) * 100
  ) %>%
  arrange(desc(Fraud_Percentage))

# Plot fraud by month
ggplot(fraud_by_month, aes(x = month_name, y = Fraud_Percentage, fill = month_name)) +
  geom_bar(stat = "identity") +
  labs(x = "Month", y = "Percentage of Fraudulent Transactions (%)",
       title = "Fraudulent Transactions by Month") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Paired")
  NULL

# Analyze fraud by hour
fraud_by_hour <- card_fraud_clean %>%
  mutate(is_fraud = as.numeric(as.character(is_fraud))) %>% 
  group_by(hour) %>%
  summarise(
    Total_Transactions = n(),
    Fraud_Transactions = sum(is_fraud),
    Fraud_Percentage = (Fraud_Transactions / Total_Transactions) * 100
  ) %>%
  arrange(desc(Fraud_Percentage))

# Plot fraud by hour
ggplot(fraud_by_hour, aes(x = hour, y = Fraud_Percentage, fill = as.factor(hour))) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Percentage of Fraudulent Transactions (%)",
       title = "Fraudulent Transactions by Hour of Day") +
  theme_bw() +
  scale_fill_viridis_d() +
  NULL

# Analyze fraud by weekday
fraud_by_weekday <- card_fraud_clean %>%
  mutate(is_fraud = as.numeric(as.character(is_fraud))) %>% 
  group_by(weekday) %>%
  summarise(
    Total_Transactions = n(),
    Fraud_Transactions = sum(is_fraud),
    Fraud_Percentage = (Fraud_Transactions / Total_Transactions) * 100
  ) %>%
  arrange(desc(Fraud_Percentage))

# Plot fraud by weekday
ggplot(fraud_by_weekday, aes(x = weekday, y = Fraud_Percentage, fill = weekday)) +
  geom_bar(stat = "identity") +
  labs(x = "Day of the Week", y = "Percentage of Fraudulent Transactions (%)",
       title = "Fraudulent Transactions by Day of the Week") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Pastel1") +
  NULL

```

```{r}
# Violin plot
ggplot(card_fraud, aes(x = factor(is_fraud), y = distance_km, fill = factor(is_fraud))) +
  geom_violin(alpha = 0.6, draw_quantiles = c(0.25, 0.5, 0.75)) +
  labs(x = "Fraud Status (0 = Legitimate, 1 = Fraudulent)", y = "Distance in Kilometers",
       title = "Distance Distribution by Fraud Status") +
  theme_bw() +
  scale_fill_manual(values = c("blue", "red"), labels = c("Legitimate", "Fraudulent")) +
  theme(legend.position = "none")
```
### Inferences from the Graphs:

1. **Distribution of Transaction Amounts (Outliers Removed)**:
   - Fraudulent transactions (in red) tend to have lower transaction amounts compared to legitimate transactions (in blue). Most fraudulent transactions are concentrated below $50.
   - Legitimate transactions have a wider range of transaction amounts, with a significant number of transactions between $0 and $50.

2. **Distribution of Continuous Variables**:
   - **Age**: Shows a relatively normal distribution with a peak around 40-60 years.
   - **amt**: Highly skewed, with most transactions having low amounts.
   - **city_pop**: Skewed distribution, indicating most transactions occur in cities with lower populations.
   - **distance_km** and **distance_miles**: Symmetrical distribution, peaking around 50-100 km/miles.
   - **lat** and **long**: Geographical data showing transactions spread across various latitudes and longitudes.

3. **Boxplots of Continuous Variables by Fraud Status**:
   - Fraudulent transactions (`is_fraud` = 1) show slightly different distributions for age and transaction amounts compared to legitimate transactions (`is_fraud` = 0).
   - The transaction amount (`amt`) for fraudulent transactions shows a lower range compared to legitimate transactions.
   - Other variables like city population, distance, and geographical coordinates do not show significant differences between fraudulent and legitimate transactions.

4. **Correlation Matrix**:
   - Strong correlation between `distance_km` and `distance_miles` (as expected since they measure the same attribute in different units).
   - Weak correlations among other variables, indicating that they are relatively independent of each other.

5. **Proportion of Fraud by Category**:
   - The proportion of fraudulent transactions is very low across all categories, with some categories like `shopping_net` and `travel` having slightly higher fraud rates.

6. **Number of Transactions Over Time**:
   - There are periodic spikes in the number of transactions, suggesting a weekly or monthly cycle.
   - A noticeable increase in transaction volume around January 2020.

7. **Distance Distribution by Fraud Status**:
   - Fraudulent transactions tend to have a slightly higher distribution of distance compared to legitimate transactions.
   - Both distributions are centered around similar values, but fraudulent transactions show a wider spread.

8. **Fraudulent Transactions by Day of the Week**:
   - Fraudulent transactions occur more frequently towards the middle of the week (Wednesday and Thursday) and slightly decrease over the weekend.

9. **Fraudulent Transactions by Hour of Day**:
   - Higher percentages of fraudulent transactions occur late in the evening and early in the morning (around 20:00 - 23:00 hours).

10. **Fraudulent Transactions by Month**:
    - There is a noticeable variation in the percentage of fraudulent transactions by month, with higher fraud rates in January and February, and lower rates around June and December.


Group all variables by type and examine each variable class by class. The dataset has the following types of variables:

1.  Strings
2.  Geospatial Data
3.  Dates
4.  Date/Times
5.  Numerical

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.

***Strings to Factors***

-   `category`, Category of Merchant
-   `job`, Job of Credit Card Holder

***Strings to Geospatial Data***

We have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location.

-   `city`, City of Credit Card Holder
-   `state`, State of Credit Card Holder

## Exploring factors: how is the compactness of categories?

-   Do we have excessive number of categories? Do we want to combine some?

```{r}
card_fraud %>% 
  count(category, sort=TRUE)%>% 
  mutate(perc = n/sum(n))

card_fraud %>% 
  count(job, sort=TRUE) %>% 
  mutate(perc = n/sum(n))


```

The predictors `category` and `job` are transformed into factors.

```{r}
#| label: convert-strings-to-factors


card_fraud <- card_fraud %>% 
  mutate(category = factor(category),
         job = factor(job))

```

`category` has 14 unique values, and `job` has 494 unique values. The dataset is quite large, with over 670K records, so these variables don't have an excessive number of levels at first glance. However, it is worth seeing if we can compact the levels to a smaller number.

### Why do we care about the number of categories and whether they are "excessive"?

Consider the extreme case where a dataset had categories that only contained one record each. There is simply insufficient data to make correct predictions using category as a predictor on new data with that category label. Additionally, if your modeling uses dummy variables, having an extremely large number of categories will lead to the production of a huge number of predictors, which can slow down the fitting. This is fine if all the predictors are useful, but if they aren't useful (as in the case of having only one record for a category), trimming them will improve the speed and quality of the data fitting.

If I had subject matter expertise, I could manually combine categories. If you don't have subject matter expertise, or if performing this task would be too labor intensive, then you can use cutoffs based on the amount of data in a category. If the majority of the data exists in only a few categories, then it might be reasonable to keep those categories and lump everything else in an "other" category or perhaps even drop the data points in smaller categories.

## Do all variables have sensible types?

Consider each variable and decide whether to keep, transform, or drop it. This is a mixture of Exploratory Data Analysis and Feature Engineering, but it's helpful to do some simple feature engineering as you explore the data. In this project, we have all data to begin with, so any transformations will be performed on the entire dataset. Ideally, do the transformations as a `recipe_step()` in the tidymodels framework. Then the transformations would be applied to any data the recipe was used on as part of the modeling workflow. There is less chance of data leakage or missing a step when you perform the feature engineering in the recipe.

## Which variables to keep in your model?

The variables "trans_date_trans_time", "city", "state", "lat", "job", "dob", "lat1_radians", "merch_lat", "merch_long", "distance_miles" and "long1_radians" are removed from "long" the data. The variables "year", "category", "amt", "city_pop", "is_fraud", "hour", "wday", "month_name", "age", "long2_radians", "lat2_radians", and "distance_km" are kept in the model

You have a number of variables and you have to decide which ones to use in your model. For instance, you have the latitude/lognitude of the customer, that of the merchant, the same data in radians, as well as the `distance_km` and `distance_miles`. Do you need them all?

## Fit your workflows in smaller sample

You will be running a series of different models, along the lines of the California housing example we have seen in class. However, this dataset has 670K rows and if you try various models and run cross validation on them, your computer may slow down or crash.

Thus, we will work with a smaller sample of 10% of the values the original dataset to identify the best model, and once we have the best model we can use the full dataset to train- test our best model.

```{r}
# select a smaller subset
my_card_fraud <- card_fraud %>% 
  # select a smaller subset, 10% of the entire dataframe 
  slice_sample(prop = 0.10) 
```

## Split the data in training - testing

```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(my_card_fraud, # updated data
                           prop = 0.8, 
                           strata = is_fraud)

card_fraud_train <- training(data_split) 
card_fraud_test <- testing(data_split)
```

## Cross Validation

Start with 3 CV folds to quickly get an estimate for the best model and you can increase the number of folds to 5 or 10 later.

```{r}
set.seed(123)
cv_folds <- vfold_cv(data = card_fraud_train, 
                          v = 3, 
                          strata = is_fraud)
cv_folds 
```

## Define a tidymodels `recipe`

What steps are you going to add to your recipe? Do you need to do any log transformations?

-   The variables `trans_date_trans_time`, `city`, `state`, `job`, and `dob` are removed from the recipe object using `step_rm()` method and these variables will not be included as a predictors in the model because these variables are unique identifier variables.

-   The categories of `Category` variable which have less than 0.05% data frequencies are converted into other using `step_other()`.

-   All the categorical variables are converted to factors using `step_noval()` and dummy variables are created using `step_dummy()`.

-   The variables with zero variance are removed using `step_zv()` and data is normalized using `step_normalize()`.

-   Finally the highly correlated variables which have correlation greater than 0.75 are removed from predictor space using `step_corr()`.

-   We will take log of the variables transaction amount `amt` and city population `city_pop` since these variables have very skewed distributions.

```{r, define_recipe}
# Remove rows with zero or negative values for 'amt' and 'city_pop'
card_fraud_train <- card_fraud_train %>%
  filter(amt > 0, city_pop > 0)

# Define recipe
fraud_rec <- recipe(is_fraud ~ ., data = card_fraud_train) %>%
  step_rm(trans_date_trans_time, city, state, job, dob) %>% 
  step_other(category, threshold = .05) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_log(city_pop) %>%
  step_log(amt) %>%
  step_normalize(all_numeric()) %>% 
  step_corr(all_predictors(), threshold = 0.75, method = "spearman") 

```

Once you have your recipe, you can check the pre-processed dataframe

```{r}
prepped_data <- 
  fraud_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)

```

## Define various models

You should define the following classification models:

1.  Logistic regression, using the `glm` engine
2.  Decision tree, using the `C5.0` engine
3.  Random Forest, using the `ranger` engine and setting `importance = "impurity"`)\
4.  A boosted tree using Extreme Gradient Boosting, and the `xgboost` engine
5.  A k-nearest neighbours, using 4 nearest_neighbors and the `kknn` engine

```{r, define_models}
## Model Building 

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 


```

## Bundle recipe and model with `workflows`

```{r, define_workflows}


## Bundle recipe and model with `workflows`


log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(fraud_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

## Work flows for other models

tree_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(tree_spec) 

rf_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(rf_spec) 

xgb_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(xgb_spec)

knn_wflow <-
  workflow() %>%
  add_recipe(fraud_rec) %>% 
  add_model(knn_spec)

```

## Fit models

You may want to compare the time it takes to fit each model. `tic()` starts a simple timer and `toc()` stops it

```{r, fit_models}
tic()
log_res <- log_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 
time <- toc()
log_time <- time[[4]]

## Decision Tree results
tic()
tree_res <-
  tree_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  ) 
time <- toc()
log_time <- time[[4]]

## Random Forest
tic()
rf_res <-
  rf_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  ) 

time <- toc()
log_time <- time[[4]]

## Boosted tree - XGBoost
tic()
xgb_res <- 
  xgb_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  ) 
time <- toc()
log_time <- time[[4]]

## K-nearest neighbour
tic()
knn_res <- 
  knn_wflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  ) 
time <- toc()
log_time <- time[[4]]


```

## Compare models

```{r, compare_models}
## Model Comparison

log_metrics <- 
  log_res %>% 
  collect_metrics() %>%
  # add the name of the model to every row
  mutate(model = "Logistic Regression") 

tree_metrics <- 
  tree_res %>% 
  collect_metrics() %>%
  mutate(model = "Decision Tree")

rf_metrics <- 
  rf_res %>% 
  collect_metrics() %>%
  mutate(model = "Random Forest")

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics() %>%
  mutate(model = "XGBoost")

knn_metrics <- 
  knn_res %>% 
  collect_metrics() %>%
  mutate(model = "Knn")

# create dataframe with all models
model_compare <- bind_rows(log_metrics,
                           tree_metrics,
                           rf_metrics,
                           xgb_metrics,
                           knn_metrics) 

#Pivot wider to create barplot
model_comp <- model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean are under the curve (ROC-AUC) for every model
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
  geom_text(
    size = 3,
    aes(label = round(mean_roc_auc, 2), 
        y = mean_roc_auc + 0.08),
    vjust = 1
  )+
  theme_light()+
  theme(legend.position = "none")+
  labs(y = NULL)


```

## Which metric to use

This is a highly imbalanced data set, as roughly 99.5% of all transactions are ok, and it's only 0.5% of transactions that are fraudulent. A `naive` model, which classifies everything as ok and not-fraud, would have an accuracy of 99.5%, but what about the sensitivity, specificity, the AUC, etc?

## `last_fit()`

```{r}

## `last_fit()` on test set

# - `last_fit()`  fits a model to the whole training data and evaluates it on the test set. 
# - provide the workflow object of the best model as well as the data split object (not the training data). 

last_fit_xgb <- last_fit(xgb_wflow, 
                        split = data_split,
                        metrics = metric_set(
                          accuracy, f_meas, kap, precision,
                          recall, roc_auc, sens, spec))

last_fit_xgb %>% collect_metrics(summarize = TRUE)

# Compare to training
xgb_res %>% collect_metrics(summarize = TRUE)


## Variable importance using `{vip}` package

library(vip)

last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  theme_light()


## Final Confusion Matrix

last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(is_fraud, .pred_class) %>% 
  autoplot(type = "heatmap")


## Final ROC curve
last_fit_xgb %>% 
  collect_predictions() %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot()
 

```

## Get variable importance using `vip` package

```{r}
library(vip)

last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  theme_light()
```

## Plot Final Confusion matrix and ROC curve

```{r}
## Final Confusion Matrix

last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(is_fraud, .pred_class) %>% 
  autoplot(type = "heatmap")


## Final ROC curve
last_fit_xgb %>% 
  collect_predictions() %>% 
  roc_curve(is_fraud, .pred_1) %>% 
  autoplot()
```

## Calculating the cost of fraud to the company

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms. Compare your model vs the naive classification that we do not have any fraudulent transactions.

```{r}
#| label: savings-for-cc-company

best_model_preds <- 
  best_model_wflow %>% 
  fit(data = card_fraud_train) %>%  
  
  ## Use `augment()` to get predictions for entire data set
  augment(new_data = card_fraud)

best_model_preds %>% 
  conf_mat(truth = is_fraud, estimate = .pred_class)

cost <- best_model_preds %>%
  select(is_fraud, amt, pred = .pred_class) 

cost <- cost %>%
  mutate(
    # naive false-- we think every single transaction is ok and not fraud
    false_naive = ifelse(is_fraud == 1, amt, 0),

    # false negatives-- we thought they were not fraud, but they were
    false_negatives = ifelse(pred == 0 & is_fraud == 1, amt, 0),

    # false positives-- we thought they were fraud, but they were not
    false_positives = ifelse(pred == 1 & is_fraud == 0, amt, 0),

    # true positives-- we thought they were fraud, and they were
    true_positives = ifelse(pred == 1 & is_fraud == 1, amt, 0),

    # true negatives-- we thought they were ok, and they were
    true_negatives = ifelse(pred == 0 & is_fraud == 0, amt, 0)
  )
  
# Summarising

cost_summary <- cost %>% 
  summarise(across(starts_with(c("false","true", "amt")), 
            ~ sum(.x, na.rm = TRUE)))

# Calculate the total amount of transactions and fraud percentage per year
annual_summary <- best_model_preds %>%
  mutate(year = year(trans_date_trans_time)) %>%
  group_by(year) %>%
  summarise(
    total_amount = sum(amt, na.rm = TRUE),
    total_fraud = sum(amt[is_fraud == 1], na.rm = TRUE),
    total_legitimate = sum(amt[is_fraud == 0], na.rm = TRUE),
    fraud_percentage = (total_fraud / total_amount) * 100
  )

# Compare the cost of the naive approach versus the model
naive_cost <- sum(cost$false_naive, na.rm = TRUE)
model_cost <- sum(cost$false_negatives, na.rm = TRUE)

# Print the results
print(cost_summary)
print(annual_summary)
cat("Naive classification cost: $", naive_cost, "\n")
cat("Model classification cost: $", model_cost, "\n")
cat("Savings using model: $", naive_cost - model_cost, "\n")

```

-   If we use a naive classifier thinking that all transactions are legitimate and not fraudulent, the cost to the company is `r scales::dollar(cost_summary$false_naives)`.

-   With our best model, the total cost of false negatives, namely transactions our classifier thinks are legitimate but which turned out to be fraud, is `r scales::dollar(cost_summary$false_negatives)`.

-   Our classifier also has some false positives, `r scales::dollar(cost_summary$false_positives)`, namely flagging transactions as fraudulent, but which were legitimate. Assuming the card company makes around 2% for each transaction (source: <https://startups.co.uk/payment-processing/credit-card-processing-fees/>), the amount of money lost due to these false positives is `r scales::dollar(cost_summary$false_positives * 0.02)`

-   The \$ improvement over the naive policy is `r scales::dollar(cost_summary$false_naives - cost_summary$false_negatives - cost_summary$false_positives * 0.02)`.
